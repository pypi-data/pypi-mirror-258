# ðŸŒ³ Dendron

### Installation

For a full installation that includes supports ðŸ¤— Transformers out of the box, run

```
pip install dendron
```

This will automatically install torch, transformers, bitsandbytes, accelerate, and sentencepiece, and protobuf. You should consider installing and using [Flash Attention](https://github.com/Dao-AILab/flash-attention), which is just a pip install, but has prerequisites that you should manually check. It's worth it though - maybe doubling your inference speeds. 

### Behavior Trees for Structured Programming of LLMs

[TODO]

## Examples

For examples of basic language model node usage, see the example notebooks in this repository. For larger and more interesting examples, see the [examples repo](https://github.com/RichardKelley/dendron-examples).

## Documentation

### Tutorials

[TODO]

### How-to Guides

[TODO]

### Explanations

[TODO]

## Acknowledgements

This work was supported in part by the Federal Transit Administration and the Regional Transportation Commission of Washoe County.