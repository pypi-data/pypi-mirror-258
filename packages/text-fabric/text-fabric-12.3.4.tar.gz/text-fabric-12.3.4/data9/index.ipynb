{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards new memory management in Text-Fabric\n",
    "\n",
    "Text-Fabric uses a lot of RAM.\n",
    "\n",
    "For a single user in a single process, this is no problem.\n",
    "\n",
    "But if several processes on the same machine need to work with the data, it becomes extremely wasteful.\n",
    "\n",
    "Example: a web server with worker processes.\n",
    "\n",
    "It would be nice to have the Text-Fabric data in **shared memory**, so that other python processes can use it.\n",
    "Then we have a database in RAM, without the usual bottlenecks of inter-process communication.\n",
    "\n",
    "[Shared memory](https://docs.python.org/3/library/multiprocessing.shared_memory.html#module-multiprocessing.shared_memory)\n",
    "is in the multiprocessing library as from Python 3.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catch phrase\n",
    "\n",
    "Build your own fast, in-memory text database with\n",
    "[array](https://docs.python.org/3/library/array.html#module-array),\n",
    "[bisect](https://docs.python.org/3/library/bisect.html#module-bisect),\n",
    "[shared memory](https://docs.python.org/3/library/multiprocessing.shared_memory.html#module-multiprocessing.shared_memory) and\n",
    "[CYTHON](https://cython.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem\n",
    "\n",
    "Shared memory consists of raw bytes. One cannot place an arbitrary Python object there.\n",
    "\n",
    "In particular, there is not yet a solution to place your dictionaries into shared memory.\n",
    "\n",
    "There is a `ShareableList` class in the multiprocessing module, but it is still immature, and it is much slower than ordinary lists. See the [python bug tracker](https://bugs.python.org/issue38891).\n",
    "\n",
    "But the generic shared memory really works, and if you are close to the metal, there is virtually no performance loss.\n",
    "See [bare.ipynb](bare.ipynb).\n",
    "\n",
    "Numpy works with shared memory, but Numpy is no good for storing strings and it does not deal with sparsity.\n",
    "If use `numpy` arrays to do the same thing as I do with `array.array`, the performance suffers a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why dictionaries?\n",
    "\n",
    "In Text-Fabric features are mappings from integers to values, so you can also store feature data in lists.\n",
    "\n",
    "However, many features are only defined for limited subranges of the nodes: the lists are sparsely filled.\n",
    "\n",
    "We need a solution for sparse lists, and dictionaries form such a solution.\n",
    "\n",
    "They are fast. However, they take up space, because the indices have to be stored as keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the metal\n",
    "\n",
    "It is possible to implement sparse lists much closer to the metal, using the\n",
    "[array](https://docs.python.org/3/library/array.html#module-array)\n",
    "module in the standard library.\n",
    "\n",
    "But then we need additional arithmetic, because feature values can have variable length.\n",
    "We have to maintain offsets and lengths for all values.\n",
    "Yet we achieve a 5-10 fold reduction in memory usage, and the `otype` feature drops from 8 MB to 180 **bytes**,\n",
    "because there is really very little information in `otype`.\n",
    "\n",
    "If we program this in pure python, we loose speed (think of a factor 7, for `otype` it is a factor 2-3).\n",
    "We can use C-extension modules for part of that, such as \n",
    "[bisect](https://docs.python.org/3/library/bisect.html#module-bisect),\n",
    "but there is still loss when we cross the boundaries between C and Python.\n",
    "\n",
    "See [sparse.py](sparse.py) for the idea and [sparse.ipynb](sparse.ipynb) for the performance test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "\n",
    "We can cythonize the logic.\n",
    "My first attempts lead to an improvement of 20%. The performance loss w.r.t. to the dictionaries is then 4 fold.\n",
    "\n",
    "See [cython.ipynb](cython.ipynb).\n",
    "\n",
    "## Considerations\n",
    "\n",
    "I just Cythonized Text-Fabric as a whole, without trying to add type declarations. \n",
    "That gave a mild performance boost: 1.5 as fast as before.\n",
    "\n",
    "See [tfcython.ipynb](tfcython.ipynb).\n",
    "\n",
    "I could try to get more out of it, especially in search, and maybe there are more aggressive speedups if both the feature access and the rest of Text-Fabric are Cythonized.\n",
    "\n",
    "## Distribution\n",
    "\n",
    "I do not know yet a good workflow to distribute a Cythonized Text-Fabric so that all users can `pip` install it.\n",
    "But it can be done, lots of other modules out there are Cythonized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opportunities\n",
    "\n",
    "## One kernel, many users\n",
    "\n",
    "Once we have a revamped memory management for text-fabric, new things come to mind:\n",
    "\n",
    "A generic text-fabric kernel could act as a database for multiple datasets.\n",
    "Via a console you can have it load or unload data.\n",
    "\n",
    "Working with Text-Fabric then means: connect with a kernel. If the kernel has that data, you can work without any lag needed for the loading.\n",
    "\n",
    "If you use the same dataset in multiple notebooks, they all use the same data.\n",
    "\n",
    "## Binary data distribution\n",
    "\n",
    "The binary data is very much shrunken. It might be a good idea to package that and attach it to a release of proper TF data. Users can then download that data and do not have to go through the process of pre-computation anymore/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work\n",
    "\n",
    "All this is a lot of work. Probably it will take two major versions, TF 9 for the new shape of the data, and TF 10 for utilizing it in a new type of TF kernel.\n",
    "\n",
    "So far, I have only experimented with node features (both integer and string).\n",
    "Edge features are more complicated, and also the precomputed data for the `L` (locality) API needs rethinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020-07-02 CC-BY Dirk Roorda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
