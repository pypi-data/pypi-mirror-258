{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ed32de",
   "metadata": {},
   "source": [
    "## Prepare train_data and test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b0c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hypergbm import make_experiment\n",
    "from hypernets.tabular.metrics import metric_to_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913bfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('datasets/Magic/train.csv.gz')\n",
    "test_data = pd.read_csv('datasets/Magic/test.csv.gz')\n",
    "X_train = train_data.copy()\n",
    "y_train = X_train.pop('Class')\n",
    "X_test = test_data.copy()\n",
    "y_test = X_test.pop('Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee1c7a",
   "metadata": {},
   "source": [
    "# Use Pseudo Labeling\n",
    "Pseudo labeling is a semi-supervised learning technique, instead of manually labeling the unlabelled data, we give approximate labels on the basis of the labelled data. Pseudo-labeling can sometimes improve the generalization capabilities of the model.\n",
    "\n",
    "**Options:**\n",
    "\n",
    "* pseudo_labeling : bool, (default=False)\n",
    "    Whether to enable pseudo labeling. Pseudo labeling is a semi-supervised learning technique, instead of manually\n",
    "    labeling the unlabelled data, we give approximate labels on the basis of the labelled data. Pseudo-labeling can\n",
    "    sometimes improve the generalization capabilities of the model.\n",
    "* pseudo_labeling_strategy : str, (default='threshold')\n",
    "    Strategy to sample pseudo labeling data(*threshold*, *number* or *quantile*).\n",
    "* pseudo_labeling_proba_threshold : float, (default=0.8)\n",
    "    Confidence threshold of pseudo-label samples. Only valid when *pseudo_labeling_strategy* is 'threshold'.\n",
    "* pseudo_labeling_proba_quantile:\n",
    "    Confidence quantile of pseudo-label samples. Only valid when *pseudo_labeling_strategy* is 'quantile'.\n",
    "* pseudo_labeling_sample_number:\n",
    "    Expected number to sample per class. Only valid when *pseudo_labeling_strategy* is 'number'.\n",
    "* pseudo_labeling_resplit : bool, (default=False)\n",
    "    Whether to re-split the training set and evaluation set after adding pseudo-labeled data. If False, the\n",
    "    pseudo-labeled data is only appended to the training set. Only valid when *pseudo_labeling* is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9abe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = make_experiment(train_data.copy(), test_data=X_test.copy(), target='Class',\n",
    "                             random_state=8888, max_trials=10, early_stopping_rounds=0,\n",
    "                             pseudo_labeling=True,\n",
    "                             )\n",
    "estimator = experiment.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e477a7-e2ca-4c61-9ee9-93c0fdf0acb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data_clean',\n",
       "  DataCleanStep(cv=True,\n",
       "                data_cleaner_args={'correct_object_dtype': True,\n",
       "                                   'drop_columns': None,\n",
       "                                   'drop_constant_columns': True,\n",
       "                                   'drop_duplicated_columns': False,\n",
       "                                   'drop_idness_columns': True,\n",
       "                                   'drop_label_nan_rows': True,\n",
       "                                   'int_convert_to': 'float', 'nan_chars': None,\n",
       "                                   'reduce_mem_usage': False,\n",
       "                                   'reserve_columns': None},\n",
       "                name='data_clean')),\n",
       " ('estimator',\n",
       "  GreedyEnsemble(weight=[0.3, 0.1, 0.05, 0.0, 0.0, 0.0, 0.15, 0.0, 0.35, 0.05], scores=[0.8835514018691589, 0.8849844236760125, 0.8857320872274144, 0.8857320872274144, 0.8854828660436137, 0.8854828660436137, 0.8857320872274144, 0.885607476635514, 0.885233644859813, 0.885233644859813, 0.8851090342679128, 0.8850467289719626, 0.8851090342679128, 0.8849844236760125, 0.884797507788162, 0.884797507788162, 0.8848598130841121, 0.8851090342679128, 0.8851090342679128, 0.8850467289719626]))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc359c8c-2429-4468-9e9e-34b3c0d4999b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7102fbcb-5d7d-46aa-804c-849956105dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7368559411146162"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = metric_to_scoring('accuracy')\n",
    "score = scorer(estimator, X_test, y_test)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
