Metadata-Version: 2.1
Name: ragrank
Version: 0.1.1
Summary: An evaluation libarary for RAG models
Home-page: https://github.com/Auto-Playground/RAGrank
License: Apache-2.0
Keywords: rag,evaluation
Author: Izam Mohammed
Author-email: izamdeveloper1@gmail.com
Requires-Python: >=3.9,<4.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: numpy (>=1.26.4,<2.0.0)
Requires-Dist: pathlib (>=1.0.1,<2.0.0)
Requires-Dist: requests (>=2.28.2,<3.0.0,!=2.30.0)
Requires-Dist: requests-toolbelt (>=1.0.0,<2.0.0)
Requires-Dist: urllib3 (>=2.2.1,<3.0.0)
Project-URL: Repository, https://github.com/Auto-Playground/RAGrank
Description-Content-Type: text/markdown

# LLM Evaluation Toolkit ðŸŽ¯

Welcome to the LLM (Large Language Model) Evaluation Toolkit! This toolkit is designed to assist you in evaluating the performance of your LLM applications. Whether you're working on chatbots, text generators, or any other natural language processing (NLP) task, our toolkit can help you assess various aspects of your model's performance.

## What does it do?

Our free LLM evaluation toolkit offers functionalities to assess:

- **Accuracy of Facts:** Evaluate how accurately your model generates factual information.
- **Understanding of Context:** Measure how well your model comprehends and maintains context within a conversation or text.
- **Tone Analysis:** Analyze the tone of generated text to ensure it aligns with your desired tone or sentiment.
- **And More:** Additional evaluation metrics and functionalities to help you gauge the effectiveness of your LLM applications.

## How to Use

1. **Installation:** Clone or download this repository to your local machine.

2. **Usage:** Utilize the provided evaluation scripts and tools to evaluate your LLM applications. Detailed instructions for each evaluation metric are included within their respective directories.

3. **Customization:** Feel free to customize and extend the toolkit to suit your specific evaluation needs. Contributions and feedback are welcomed!

## Contributing

If you'd like to contribute to the LLM Evaluation Toolkit, please follow these steps:

1. Fork this repository.
2. Create a new branch (`git checkout -b feature/improvement`).
3. Make your changes.
4. Commit your changes (`git commit -am 'Add new feature'`).
5. Push to the branch (`git push origin feature/improvement`).
6. Create a new Pull Request.

## License

This project is licensed under the [Apatche Licence](LICENSE).

## Feedback and Support

If you encounter any issues, have questions, or would like to provide feedback, please open an issue on the GitHub repository.

Happy evaluating!

